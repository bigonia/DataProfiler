# 文件数据源优化设计方案

## 1. 概述

### 1.1 优化目标

本方案旨在优化当前文件数据源的设计架构，实现以下核心目标：

1. **分离关注点**：将文件上传和数据源创建解耦，提高系统的灵活性和可维护性
2. **简化配置**：使用系统级数据库配置，基于Spring多数据源管理
3. **无状态转换**：转换功能设计为无状态，通过文件元数据字段管理转换状态
4. **标准化映射**：建立filename-schema、sheet-table的标准映射关系
5. **增强扩展性**：为未来支持更多文件格式奠定基础

### 1.2 当前架构问题分析

**现有问题**：
- 文件上传与数据源创建紧耦合，缺乏灵活性
- 缺乏统一的文件到数据库映射规范
- 转换逻辑过于复杂，难以维护
- 文件转换状态管理不清晰

## 2. 架构设计

### 2.1 整体架构

```
┌─────────────────┐    ┌──────────────────┐    ┌─────────────────────┐
│   File Upload   │    │  File Management │    │  File Conversion    │
│   Controller    │───▶│     Service      │───▶│     Utility         │
└─────────────────┘    └──────────────────┘    └─────────────────────┘
         │                        │                         │
         │                        │                         ▼
         │                        │              ┌─────────────────────┐
         │                        │              │  System Database    │
         │                        │              │  Configuration      │
         │                        │              │  (Spring Config)    │
         │                        │              └─────────────────────┘
         │                        │                         │
         │                        │                         ▼
         │                        │              ┌─────────────────────┐
         │                        │              │  Schema & Table     │
         │                        │              │    Generator        │
         │                        │              └─────────────────────┘
         │                        │                         │
         │                        ▼                         ▼
         │              ┌──────────────────┐    ┌─────────────────────┐
         │              │  File Metadata   │    │   System Database   │
         │              │  (with conversion│    │   (configured via   │
         │              │   status fields) │    │   application.yml)  │
         │              └──────────────────┘    └─────────────────────┘
         ▼
┌─────────────────┐
│  File Storage   │
│   (uploads/)    │
└─────────────────┘
```

### 2.2 核心组件设计

#### 2.2.1 文件管理服务 (FileManagementService)

**职责**：
- 处理文件上传和存储
- 管理文件元数据（包含转换状态字段）
- 提供文件访问和管理接口
- 触发文件转换操作

**接口定义**：
```java
public interface FileManagementService {
    
    /**
     * Upload and store file
     * @param file Uploaded file
     * @param metadata Additional file metadata
     * @return FileMetadata with unique file ID
     */
    FileMetadata uploadFile(MultipartFile file, Map<String, String> metadata);
    
    /**
     * Convert file to data source
     * @param fileId Unique file identifier
     * @param dataSourceName Custom data source name
     * @return DataSourceConfig for the converted data source
     */
    DataSourceConfig convertFileToDataSource(String fileId, String dataSourceName);
    
    /**
     * Get file metadata by file ID
     * @param fileId Unique file identifier
     * @return FileMetadata
     */
    FileMetadata getFileMetadata(String fileId);
    
    /**
     * Delete file and metadata
     * @param fileId Unique file identifier
     */
    void deleteFile(String fileId);
}```

#### 2.2.2 文件转换工具 (FileConversionUtil)

**职责**：
- 将文件转换为数据库表结构
- 使用系统配置的数据库连接
- 执行数据加载
- 生成数据源配置

**工具类定义**：
```java
@Component
public class FileConversionUtil {
    
    /**
     * Convert file to data source using system database
     * @param fileId File identifier
     * @param filePath File path
     * @param dataSourceName Custom data source name
     * @return DataSourceConfig for the converted data source
     */
    public DataSourceConfig convertFileToDataSource(String fileId, String filePath, String dataSourceName);
    
    /**
     * Get supported file formats
     * @return List of supported file extensions
     */
    public List<String> getSupportedFormats();
    
    /**
     * Check if file format is supported
     * @param filename File name
     * @return true if supported
     */
    public boolean isFormatSupported(String filename);
}
```

#### 2.2.3 文件元数据扩展 (FileMetadata)

**数据结构**：
```java
@Data
@Entity
@Table(name = "file_metadata")
public class FileMetadata {
    @Id
    private String fileId;
    private String originalFilename;
    private String filePath;
    private Long fileSize;
    private String mimeType;
    private LocalDateTime uploadedAt;
    
    // 转换状态相关字段
    private Boolean converted = false;           // 是否已转换
    private String dataSourceId;                 // 关联的数据源ID
    private LocalDateTime convertedAt;           // 转换时间
    private String conversionError;              // 转换错误信息
    private Integer tableCount;                  // 生成的表数量
    
    // 其他元数据
    private String description;
    private String tags;
}
```

#### 2.2.4 系统数据库配置 (application.yml)

**配置示例**：
```yaml
spring:
  datasource:
    # 主数据源（系统数据库）
    primary:
      url: jdbc:sqlite:data/core.db
      driver-class-name: org.sqlite.JDBC
    
    # 文件转换目标数据源（可选，默认使用主数据源）
    file-conversion:
      url: jdbc:mysql://localhost:3306/file_data
      username: ${DB_USERNAME:root}
      password: ${DB_PASSWORD:password}
      driver-class-name: com.mysql.cj.jdbc.Driver

# 文件转换配置
file:
  conversion:
    target-datasource: file-conversion  # 目标数据源名称，默认为primary
    schema-prefix: "file_"              # Schema前缀
    table-prefix: "tbl_"                # 表前缀
    batch-size: 1000                    # 批处理大小
    create-indexes: true                # 是否自动创建索引
```

### 2.3 映射关系设计

#### 2.3.1 Filename-Schema 映射

**规则**：
```
Schema Name = [schemaPrefix_]filename_without_extension_[timestamp]

示例：
- sales_data.xlsx → schema: sales_data_20250125
- customer_info.csv → schema: customer_info_20250125
- 带前缀: prefix_sales_data_20250125
```

#### 2.3.2 Sheet-Table 映射

**规则**：
```
Table Name = [tablePrefix_]sheet_name_sanitized

示例：
- Excel Sheet "Sales Q1" → table: sales_q1
- Excel Sheet "产品信息" → table: product_info (transliterated)
- CSV文件 → table: data (default name)
- 带前缀: prefix_sales_q1
```

**名称清理规则**：
- 移除特殊字符，保留字母、数字、下划线
- 转换为小写
- 中文字符转换为拼音或英文等价词
- 长度限制在64字符以内
- 避免SQL关键字冲突

## 3. 详细实现方案

### 3.1 API接口设计

#### 3.1.1 文件上传接口

```http
POST /api/files/upload
Content-Type: multipart/form-data

Parameters:
- file: MultipartFile (required)
- description: String (optional)
- tags: String (optional)

Response:
{
  "fileId": "uuid-string",
  "originalFilename": "sales_data.xlsx",
  "fileSize": 1024000,
  "mimeType": "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
  "uploadedAt": "2025-01-25T10:30:00Z",
  "converted": false,
  "dataSourceId": null
}
```

#### 3.1.2 文件转换接口

```http
POST /api/files/{fileId}/convert
Content-Type: application/json

Request Body:
{
  "dataSourceName": "Sales Data Q1 2025",
  "description": "Quarterly sales report"
}

Response:
{
  "dataSourceId": "ds_uuid",
  "dataSourceName": "Sales Data Q1 2025",
  "createdTables": [
    {
      "schemaName": "file_sales_data_20250125",
      "tableName": "tbl_sales_q1",
      "rowCount": 15000,
      "columnCount": 8
    },
    {
      "schemaName": "file_sales_data_20250125",
      "tableName": "tbl_products",
      "rowCount": 500,
      "columnCount": 4
    }
  ],
  "convertedAt": "2025-01-25T10:38:30Z"
}
```

#### 3.1.3 文件信息查询接口

```http
GET /api/files/{fileId}

Response:
{
  "fileId": "uuid-string",
  "originalFilename": "sales_data.xlsx",
  "fileSize": 1024000,
  "mimeType": "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
  "uploadedAt": "2025-01-25T10:30:00Z",
  "converted": true,
  "dataSourceId": "ds_uuid",
  "convertedAt": "2025-01-25T10:38:30Z",
  "tableCount": 2,
  "conversionError": null,
  "description": "Quarterly sales report",
  "tags": "sales,q1,2025"
}
```

### 3.2 文件处理实现

#### 3.2.1 Excel文件处理

```java
@Component
public class ExcelFileProcessor {
    
    @Autowired
    private DataSource fileConversionDataSource;
    
    @Autowired
    private DatabaseUtil databaseUtil;
    
    @Value("${file.conversion.schema-prefix:file_}")
    private String schemaPrefix;
    
    @Value("${file.conversion.table-prefix:tbl_}")
    private String tablePrefix;
    
    @Value("${file.conversion.batch-size:1000}")
    private int batchSize;
    
    public List<TableInfo> processExcelFile(String fileId, String filePath, String dataSourceName) {
        List<TableInfo> createdTables = new ArrayList<>();
        
        try (InputStream fileStream = new FileInputStream(filePath);
             Connection connection = fileConversionDataSource.getConnection()) {
            
            Workbook workbook = WorkbookFactory.create(fileStream);
            String schemaName = generateSchemaName(dataSourceName);
            
            // Create schema if not exists
            databaseUtil.createSchemaIfNotExists(connection, schemaName);
            
            for (Sheet sheet : workbook) {
                TableInfo tableInfo = processSheet(sheet, connection, schemaName);
                createdTables.add(tableInfo);
            }
            
        } catch (Exception e) {
            throw new ConversionException("Failed to process Excel file: " + e.getMessage(), e);
        }
        
        return createdTables;
    }
    
    private TableInfo processSheet(Sheet sheet, Connection connection, String schemaName) {
        String tableName = generateTableName(sheet.getSheetName());
        
        // Analyze columns from header row
        List<ColumnDefinition> columns = analyzeColumns(sheet.getRow(0));
        
        // Create table
        String createTableSql = databaseUtil.generateCreateTableSql(schemaName, tableName, columns, connection);
        try (Statement stmt = connection.createStatement()) {
            stmt.execute(createTableSql);
        }
        
        // Insert data
        int rowCount = insertData(sheet, connection, schemaName, tableName, columns);
        
        return new TableInfo(schemaName, tableName, rowCount, columns.size());
    }
    
    private List<ColumnDefinition> analyzeColumns(Row headerRow) {
        List<ColumnDefinition> columns = new ArrayList<>();
        
        if (headerRow != null) {
            for (int i = 0; i < headerRow.getLastCellNum(); i++) {
                Cell cell = headerRow.getCell(i);
                String columnName = getCellValueAsString(cell);
                
                if (StringUtils.isBlank(columnName)) {
                    columnName = "column_" + (i + 1);
                }
                
                columns.add(new ColumnDefinition(
                    sanitizeColumnName(columnName), 
                    DataType.VARCHAR, 
                    true
                ));
            }
        }
        
        return columns;
    }
    
    private int insertData(Sheet sheet, Connection connection, String schemaName, 
                          String tableName, List<ColumnDefinition> columns) throws SQLException {
        
        String insertSql = buildInsertSql(schemaName, tableName, columns, connection);
        int rowCount = 0;
        
        try (PreparedStatement pstmt = connection.prepareStatement(insertSql)) {
            int batchCount = 0;
            
            // 跳过表头，从第二行开始处理数据
            for (int rowIndex = 1; rowIndex <= sheet.getLastRowNum(); rowIndex++) {
                Row row = sheet.getRow(rowIndex);
                if (row == null) continue;
                
                // 设置参数
                for (int colIndex = 0; colIndex < columns.size(); colIndex++) {
                    Cell cell = row.getCell(colIndex);
                    String cellValue = getCellValueAsString(cell);
                    pstmt.setString(colIndex + 1, cellValue);
                }
                
                pstmt.addBatch();
                batchCount++;
                rowCount++;
                
                // 批量执行
                if (batchCount >= batchSize) {
                    pstmt.executeBatch();
                    batchCount = 0;
                }
            }
            
            // 执行剩余的批次
            if (batchCount > 0) {
                pstmt.executeBatch();
            }
        }
        
        return rowCount;
    }
    
    private String buildInsertSql(String schemaName, String tableName, 
                                List<ColumnDefinition> columns, Connection connection) throws SQLException {
        DatabaseMetaData metaData = connection.getMetaData();
        String databaseType = metaData.getDatabaseProductName().toLowerCase();
        
        StringBuilder sql = new StringBuilder();
        
        if (databaseType.contains("sqlite")) {
            sql.append("INSERT INTO \"").append(tableName).append("\" (");
        } else {
            sql.append("INSERT INTO ").append(schemaName).append(".").append(tableName).append(" (");
        }
        
        for (int i = 0; i < columns.size(); i++) {
            sql.append(columns.get(i).getName());
            if (i < columns.size() - 1) {
                sql.append(", ");
            }
        }
        
        sql.append(") VALUES (");
        
        for (int i = 0; i < columns.size(); i++) {
            sql.append("?");
            if (i < columns.size() - 1) {
                sql.append(", ");
            }
        }
        
        sql.append(")");
        return sql.toString();
    }
    
    private String getCellValueAsString(Cell cell) {
        if (cell == null) {
            return "";
        }
        
        switch (cell.getCellType()) {
            case STRING:
                return cell.getStringCellValue();
            case NUMERIC:
                if (DateUtil.isCellDateFormatted(cell)) {
                    return cell.getDateCellValue().toString();
                } else {
                    return String.valueOf(cell.getNumericCellValue());
                }
            case BOOLEAN:
                return String.valueOf(cell.getBooleanCellValue());
            case FORMULA:
                return cell.getCellFormula();
            default:
                return "";
        }
    }
    
    private String sanitizeColumnName(String columnName) {
        return columnName.replaceAll("[^a-zA-Z0-9_]", "_")
                        .replaceAll("^[0-9]", "col_$0")
                        .toLowerCase();
    }
    
    private String generateSchemaName(String dataSourceName) {
        return schemaPrefix + sanitizeColumnName(dataSourceName) + "_" + System.currentTimeMillis();
    }
    
    private String generateTableName(String sheetName) {
        return tablePrefix + sanitizeColumnName(sheetName);
    }
}
```

#### 3.2.2 CSV文件处理

```java
@Component
public class CsvFileProcessor {
    
    @Autowired
    private DataSource fileConversionDataSource;
    
    @Autowired
    private DatabaseUtil databaseUtil;
    
    @Value("${file.conversion.schema-prefix:file_}")
    private String schemaPrefix;
    
    @Value("${file.conversion.table-prefix:tbl_}")
    private String tablePrefix;
    
    @Value("${file.conversion.batch-size:1000}")
    private int batchSize;
    
    public List<TableInfo> processCsvFile(String fileId, String filePath, String dataSourceName) {
        List<TableInfo> createdTables = new ArrayList<>();
        
        try (BufferedReader reader = new BufferedReader(new FileReader(filePath));
             Connection connection = fileConversionDataSource.getConnection()) {
            
            String schemaName = generateSchemaName(dataSourceName);
            String tableName = generateTableName("data");
            
            // Create schema if not exists
            databaseUtil.createSchemaIfNotExists(connection, schemaName);
            
            // Read header line
            String headerLine = reader.readLine();
            if (headerLine == null) {
                throw new ConversionException("CSV file is empty");
            }
            
            // Analyze columns from header
            List<ColumnDefinition> columns = parseColumns(headerLine);
            
            // Create table
            String createTableSql = databaseUtil.generateCreateTableSql(schemaName, tableName, columns, connection);
            try (Statement stmt = connection.createStatement()) {
                stmt.execute(createTableSql);
            }
            
            // Insert data
            int rowCount = insertCsvData(reader, connection, schemaName, tableName, columns);
            
            createdTables.add(new TableInfo(schemaName, tableName, rowCount, columns.size()));
            
        } catch (Exception e) {
            throw new ConversionException("Failed to process CSV file: " + e.getMessage(), e);
        }
        
        return createdTables;
    }
    
    private List<ColumnDefinition> parseColumns(String headerLine) {
        List<String> columnNames = parseCsvLine(headerLine);
        List<ColumnDefinition> columns = new ArrayList<>();
        
        for (int i = 0; i < columnNames.size(); i++) {
            String columnName = columnNames.get(i);
            if (StringUtils.isBlank(columnName)) {
                columnName = "column_" + (i + 1);
            }
            
            columns.add(new ColumnDefinition(
                sanitizeColumnName(columnName), 
                DataType.VARCHAR, 
                true
            ));
        }
        
        return columns;
    }
    
    private int insertCsvData(BufferedReader reader, Connection connection, String schemaName, 
                             String tableName, List<ColumnDefinition> columns) throws IOException, SQLException {
        
        String insertSql = buildInsertSql(schemaName, tableName, columns, connection);
        int rowCount = 0;
        
        try (PreparedStatement pstmt = connection.prepareStatement(insertSql)) {
            String line;
            int batchCount = 0;
            
            while ((line = reader.readLine()) != null) {
                List<String> values = parseCsvLine(line);
                
                // 设置参数
                for (int i = 0; i < columns.size(); i++) {
                    String value = i < values.size() ? values.get(i) : "";
                    pstmt.setString(i + 1, value);
                }
                
                pstmt.addBatch();
                batchCount++;
                rowCount++;
                
                // 批量执行
                if (batchCount >= batchSize) {
                    pstmt.executeBatch();
                    batchCount = 0;
                }
            }
            
            // 执行剩余的批次
            if (batchCount > 0) {
                pstmt.executeBatch();
            }
        }
        
        return rowCount;
    }
    
    private String buildInsertSql(String schemaName, String tableName, 
                                List<ColumnDefinition> columns, Connection connection) throws SQLException {
        DatabaseMetaData metaData = connection.getMetaData();
        String databaseType = metaData.getDatabaseProductName().toLowerCase();
        
        StringBuilder sql = new StringBuilder();
        
        if (databaseType.contains("sqlite")) {
            sql.append("INSERT INTO \"").append(tableName).append("\" (");
        } else {
            sql.append("INSERT INTO ").append(schemaName).append(".").append(tableName).append(" (");
        }
        
        for (int i = 0; i < columns.size(); i++) {
            sql.append(columns.get(i).getName());
            if (i < columns.size() - 1) {
                sql.append(", ");
            }
        }
        
        sql.append(") VALUES (");
        
        for (int i = 0; i < columns.size(); i++) {
            sql.append("?");
            if (i < columns.size() - 1) {
                sql.append(", ");
            }
        }
        
        sql.append(")");
        return sql.toString();
    }
    
    private List<String> parseCsvLine(String line) {
        List<String> result = new ArrayList<>();
        
        boolean inQuotes = false;
        StringBuilder currentField = new StringBuilder();
        
        for (int i = 0; i < line.length(); i++) {
            char c = line.charAt(i);
            
            if (c == '"') {
                inQuotes = !inQuotes;
            } else if (c == ',' && !inQuotes) {
                result.add(currentField.toString().trim());
                currentField.setLength(0);
            } else {
                currentField.append(c);
            }
        }
        
        result.add(currentField.toString().trim());
        return result;
    }
    
    private String sanitizeColumnName(String columnName) {
        return columnName.replaceAll("[^a-zA-Z0-9_]", "_")
                        .replaceAll("^[0-9]", "col_$0")
                        .toLowerCase();
    }
    
    private String generateSchemaName(String dataSourceName) {
        return schemaPrefix + sanitizeColumnName(dataSourceName) + "_" + System.currentTimeMillis();
    }
    
    private String generateTableName(String name) {
        return tablePrefix + sanitizeColumnName(name);
    }
}
```

### 3.3 数据源配置

#### 3.3.1 Spring多数据源配置

```java
@Configuration
@EnableConfigurationProperties(FileConversionProperties.class)
public class DataSourceConfiguration {
    
    @Primary
    @Bean(name = "primaryDataSource")
    @ConfigurationProperties("spring.datasource.primary")
    public DataSource primaryDataSource() {
        return DataSourceBuilder.create().build();
    }
    
    @Bean(name = "fileConversionDataSource")
    @ConfigurationProperties("spring.datasource.file-conversion")
    public DataSource fileConversionDataSource() {
        return DataSourceBuilder.create().build();
    }
    
    @Bean
    public DataSource getTargetDataSource(FileConversionProperties properties) {
        String targetDataSourceName = properties.getTargetDatasource();
        if ("file-conversion".equals(targetDataSourceName)) {
            return fileConversionDataSource();
        }
        return primaryDataSource(); // 默认使用主数据源
    }
}

@ConfigurationProperties(prefix = "file.conversion")
@Data
public class FileConversionProperties {
    private String targetDatasource = "primary";
    private String schemaPrefix = "file_";
    private String tablePrefix = "tbl_";
    private int batchSize = 1000;
    private boolean createIndexes = true;
}
```

#### 3.3.2 数据库工具类

```java
@Component
public class DatabaseUtil {
    
    @Autowired
    private FileConversionProperties properties;
    
    /**
     * 创建Schema（如果不存在）
     */
    public void createSchemaIfNotExists(Connection connection, String schemaName) throws SQLException {
        DatabaseMetaData metaData = connection.getMetaData();
        String databaseType = metaData.getDatabaseProductName().toLowerCase();
        
        String sql;
        if (databaseType.contains("mysql")) {
            sql = "CREATE SCHEMA IF NOT EXISTS `" + schemaName + "`";
        } else if (databaseType.contains("postgresql")) {
            sql = "CREATE SCHEMA IF NOT EXISTS \"" + schemaName + "\"";
        } else {
            // SQLite不支持Schema，跳过
            return;
        }
        
        try (Statement stmt = connection.createStatement()) {
            stmt.execute(sql);
        }
    }
    
    /**
     * 生成CREATE TABLE SQL
     */
    public String generateCreateTableSql(String schemaName, String tableName, 
                                       List<ColumnDefinition> columns, Connection connection) throws SQLException {
        DatabaseMetaData metaData = connection.getMetaData();
        String databaseType = metaData.getDatabaseProductName().toLowerCase();
        
        StringBuilder sql = new StringBuilder();
        
        if (databaseType.contains("sqlite")) {
            sql.append("CREATE TABLE IF NOT EXISTS \"").append(tableName).append("\" (\n");
        } else {
            String fullTableName = schemaName + "." + tableName;
            sql.append("CREATE TABLE IF NOT EXISTS ").append(fullTableName).append(" (\n");
        }
        
        for (int i = 0; i < columns.size(); i++) {
            ColumnDefinition col = columns.get(i);
            sql.append("  ").append(col.getName()).append(" ")
               .append(mapDataType(col.getType(), databaseType));
            
            if (!col.isNullable()) {
                sql.append(" NOT NULL");
            }
            
            if (i < columns.size() - 1) {
                sql.append(",");
            }
            sql.append("\n");
        }
        
        sql.append(")");
        
        // MySQL特定配置
        if (databaseType.contains("mysql")) {
            sql.append(" ENGINE=InnoDB DEFAULT CHARSET=utf8mb4");
        }
        
        return sql.toString();
    }
    
    /**
     * 数据类型映射
     */
    private String mapDataType(DataType genericType, String databaseType) {
        if (databaseType.contains("mysql")) {
            return mapMySqlDataType(genericType);
        } else if (databaseType.contains("postgresql")) {
            return mapPostgreSqlDataType(genericType);
        } else {
            return mapSqliteDataType(genericType);
        }
    }
    
    private String mapMySqlDataType(DataType type) {
        switch (type) {
            case INTEGER: return "INT";
            case LONG: return "BIGINT";
            case DOUBLE: return "DOUBLE";
            case DECIMAL: return "DECIMAL(19,4)";
            case DATE: return "DATE";
            case DATETIME: return "DATETIME";
            case BOOLEAN: return "BOOLEAN";
            case TEXT: return "TEXT";
            case VARCHAR: return "VARCHAR(255)";
            default: return "TEXT";
        }
    }
    
    private String mapPostgreSqlDataType(DataType type) {
        switch (type) {
            case INTEGER: return "INTEGER";
            case LONG: return "BIGINT";
            case DOUBLE: return "DOUBLE PRECISION";
            case DECIMAL: return "DECIMAL(19,4)";
            case DATE: return "DATE";
            case DATETIME: return "TIMESTAMP";
            case BOOLEAN: return "BOOLEAN";
            case TEXT: return "TEXT";
            case VARCHAR: return "VARCHAR(255)";
            default: return "TEXT";
        }
    }
    
    private String mapSqliteDataType(DataType type) {
        switch (type) {
            case INTEGER: return "INTEGER";
            case LONG: return "INTEGER";
            case DOUBLE: return "REAL";
            case DECIMAL: return "REAL";
            case DATE: return "TEXT";
            case DATETIME: return "TEXT";
            case BOOLEAN: return "INTEGER";
            case TEXT: return "TEXT";
            case VARCHAR: return "TEXT";
            default: return "TEXT";
        }
    }
}
```

## 4. 合理性分析

### 4.1 架构优势

#### 4.1.1 分离关注点
- **文件管理**与**数据转换**职责清晰分离
- 支持文件上传后的多次转换和配置
- 便于独立测试和维护

#### 4.1.2 灵活的目标数据库配置
- 支持多种数据库类型（MySQL、PostgreSQL、SQLite、Oracle等）
- 可配置的连接参数和认证信息
- 默认回退到系统SQLite，保持向后兼容

#### 4.1.3 流式处理能力
- 支持大文件的内存友好处理
- 批量插入提高性能
- 可配置的批处理大小

#### 4.1.4 标准化映射规则
- 一致的命名规范
- 可自定义的前缀和映射规则
- 避免命名冲突的机制

### 4.2 潜在问题与风险

#### 4.2.1 复杂性增加
**问题**：架构变得更加复杂，增加了开发和维护成本

**缓解措施**：
- 提供清晰的接口文档和示例
- 实现完善的错误处理和日志记录
- 建立自动化测试覆盖

#### 4.2.2 性能考虑
**问题**：多步骤处理可能影响整体性能

**缓解措施**：
- 异步处理转换任务
- 实现进度跟踪和状态监控
- 优化数据库连接和批处理策略

#### 4.2.3 数据一致性
**问题**：分离的架构可能导致数据不一致

**缓解措施**：
- 实现事务管理和回滚机制
- 添加数据完整性检查
- 提供数据修复和清理工具

#### 4.2.4 安全性考虑
**问题**：支持外部数据库连接增加安全风险

**缓解措施**：
- 实现连接参数加密存储
- 添加访问权限控制
- 支持SSL/TLS连接
- 实现连接池和超时管理

### 4.3 扩展性评估

#### 4.3.1 文件格式扩展
- 插件化的文件处理器架构
- 易于添加新的文件格式支持
- 统一的处理接口

#### 4.3.2 数据库类型扩展
- 适配器模式支持新数据库类型
- 标准化的DDL生成接口
- 可配置的类型映射

#### 4.3.3 功能扩展
- 支持数据转换和清洗规则
- 可扩展的元数据管理
- 集成数据质量检查

## 5. 技术实现亮点

### 5.1 简化的架构设计
- **职责明确**: FileManagementService专注文件管理，FileConversionUtil负责转换逻辑
- **无状态转换**: 转换功能设计为无状态，通过文件元数据字段管理转换状态
- **系统级配置**: 基于Spring多数据源管理，使用系统级配置而非外部传入

### 5.2 Spring多数据源集成
- **配置驱动**: 通过application.yml配置目标数据源
- **自动装配**: 利用Spring的@ConfigurationProperties自动绑定配置
- **默认回退**: 支持缺省使用当前系统DB配置

### 5.3 高效的文件处理
- **批量插入**: 使用PreparedStatement批量插入提升性能
- **内存优化**: 逐行处理文件内容，避免大文件内存溢出
- **事务管理**: 通过数据库事务确保数据一致性

### 5.4 数据库兼容性
- **多数据库支持**: 通过DatabaseUtil工具类支持MySQL、PostgreSQL、SQLite
- **动态SQL生成**: 根据数据库类型生成相应的DDL和DML语句
- **类型映射**: 自动处理不同数据库的数据类型差异

### 5.5 灵活的配置管理
- **可配置参数**: 支持批处理大小、Schema前缀、表前缀等配置
- **环境适配**: 通过Spring Profile支持不同环境配置
- **热配置**: 支持运行时配置更新

### 5.6 简洁的API设计
- **RESTful接口**: 遵循REST设计原则，接口语义清晰
- **状态查询**: 通过文件信息接口直接查询转换状态
- **错误处理**: 统一的异常处理和错误信息返回

## 6. 实施计划

### 6.1 阶段一：核心架构实现（1-2周）

**目标**：实现简化的分离架构

**任务**：
1. 修改FileManagementService接口，增加转换相关方法
2. 创建FileConversionUtil工具类
3. 实现DatabaseUtil数据库工具类
4. 配置Spring多数据源管理
5. 扩展FileMetadata实体，增加转换状态字段

**交付物**：
- 简化的服务接口和实现
- 数据库工具类
- 基础的单元测试

### 6.2 阶段二：文件处理实现（1-2周）

**目标**：实现高效的文件处理和数据库操作

**任务**：
1. 实现ExcelFileProcessor和CsvFileProcessor
2. 优化批量插入和事务管理
3. 实现多数据库兼容性处理
4. 添加文件转换状态管理
5. 实现错误处理和回滚机制

**交付物**：
- 文件处理器实现
- 多数据库适配逻辑
- 完善的错误处理机制

### 6.3 阶段三：API接口更新（1周）

**目标**：更新API接口支持新的处理流程

**任务**：
1. 更新FileController支持分离的上传和转换流程
2. 实现文件转换接口
3. 实现文件信息查询接口（包含转换状态）
4. 移除复杂的转换状态管理接口
5. 更新API文档

**交付物**：
- 简化的API接口
- 更新的API文档
- 接口测试用例

### 6.4 阶段四：配置和测试（1-2周）

**目标**：完善配置管理和测试覆盖

**任务**：
1. 完善application.yml配置
2. 单元测试和集成测试
3. 多数据库兼容性测试
4. 错误处理和异常情况测试
5. 性能基准测试

**交付物**：
- 完整的配置文件
- 完善的测试覆盖
- 性能基准报告

### 6.5 阶段五：代码清理和部署（1周）

**目标**：代码优化和生产部署

**任务**：
1. 删除废弃的类和方法
2. 代码审查和文档更新
3. 生产环境部署
4. 功能验证和监控配置
5. 用户培训和迁移指南

**交付物**：
- 清理后的代码库
- 完善的技术文档
- 部署和迁移指南

## 6. 风险评估与缓解

### 6.1 技术风险

| 风险 | 影响 | 概率 | 缓解措施 |
|------|------|------|----------|
| 流式处理内存泄漏 | 高 | 中 | 严格的资源管理，自动化内存监控 |
| 数据库连接池耗尽 | 高 | 中 | 连接池配置优化，超时和重试机制 |
| 大文件处理超时 | 中 | 高 | 异步处理，进度跟踪，断点续传 |
| 数据类型映射错误 | 中 | 中 | 完善的类型推断，用户确认机制 |

### 6.2 业务风险

| 风险 | 影响 | 概率 | 缓解措施 |
|------|------|------|----------|
| 向后兼容性问题 | 高 | 低 | 保留原有API，渐进式迁移 |
| 用户学习成本 | 中 | 中 | 详细文档，示例代码，培训支持 |
| 性能回退 | 中 | 低 | 性能基准测试，持续监控 |

### 6.3 运维风险

| 风险 | 影响 | 概率 | 缓解措施 |
|------|------|------|----------|
| 配置复杂性增加 | 中 | 高 | 默认配置，配置验证，管理界面 |
| 故障排查难度 | 中 | 中 | 详细日志，监控告警，诊断工具 |
| 数据安全风险 | 高 | 低 | 加密存储，访问控制，审计日志 |

## 7. 总结

### 7.1 方案优势

1. **架构清晰**：分离关注点，提高系统的可维护性和扩展性
2. **灵活配置**：支持多种目标数据库，满足不同业务需求
3. **高性能**：流式处理支持大文件，批处理提高效率
4. **标准化**：统一的映射规则，避免命名冲突
5. **可扩展**：插件化架构，易于添加新功能

### 7.2 实施建议

1. **渐进式实施**：分阶段实施，降低风险
2. **向后兼容**：保留原有功能，平滑迁移
3. **充分测试**：完善的测试覆盖，确保质量
4. **监控完善**：实时监控，快速响应问题
5. **文档齐全**：详细的技术文档和用户指南

### 7.3 预期收益

1. **开发效率提升**：清晰的架构降低开发复杂度
2. **系统性能改善**：流式处理和批量操作提高性能
3. **业务灵活性增强**：支持多种数据库和配置选项
4. **维护成本降低**：模块化设计便于维护和扩展
5. **用户体验优化**：更好的进度跟踪和错误处理

本方案通过系统性的架构优化，将显著提升文件数据源处理的灵活性、性能和可维护性，为系统的长期发展奠定坚实基础。